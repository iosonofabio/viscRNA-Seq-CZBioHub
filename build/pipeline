#!/usr/bin/env python
'''
viscRNA-Seq pipeline local script.

This script runs on the newly allocated EC2 instance as directed by the viscrna-seq-pipeline script and does the heavy lifting.
'''
import os
import sys
import shutil
import subprocess as sp
import argparse


class Pipeline:
    def __init__(
            self,
            input_data_path,
            input_samplesheet_path,
            output_path,
            output_gdrive_path,
            dataset_id,
            samplenames,
            chemistry,
            expect_cells,
            ):

        self.input_data_path = input_data_path
        self.input_samplesheet_path = input_samplesheet_path
        self.output_path = output_path
        self.output_gdrive_path = output_gdrive_path
        self.dataset_id = dataset_id
        self.samplenames = samplenames
        self.chemistry = chemistry
        self.expect_cells = expect_cells

    def remove_old_data(self):
        shutil.rmtree('/data/{:}'.format(self.dataset_id), ignore_errors=True)
        shutil.rmtree('/home/ubuntu/{:}'.format(self.dataset_id), ignore_errors=True)
        shutil.rmtree('/home/ubuntu/__{:}.mro'.format(self.dataset_id), ignore_errors=True)

    def make_data_folders(self):
        os.makedirs('/data/{:}'.format(self.dataset_id), exist_ok=True)
        os.makedirs('/data/{:}/bcl'.format(self.dataset_id), exist_ok=True)
        os.makedirs('/data/{:}/log'.format(self.dataset_id), exist_ok=True)
        os.makedirs('/data/{:}/count'.format(self.dataset_id), exist_ok=True)

    def get_data(self, path):
        if path.startswith('s3://'):
            return self.get_data_s3(path)
        elif path.startswith('http://') or path.startswith('https://'):
            return self.get_data_url(path)
        else:
            raise ValueError('Type of data source not recognized: {:}'.format(path))

    def unpack_data(self, path):
        parent = '/data/{:}/bcl'.format(self.dataset_id)
        fn = '/data/{:}/bcl/{:}'.format(self.dataset_id, os.path.basename(path))

        if fn.endswith('.tar.gz') or fn.endswith('.tar.xz'):
            sp.run(' '.join([
                'tar', '-xf', path, '-C', parent,
                ]),
                shell=True,
                check=True)
        else:
            raise ValueError('Compressed format not supported: {:}'.format(
                fn.split('.')[-1]))

        os.remove(path)

    def get_data_s3(self, url):
        fn = '/data/{:}/bcl/{:}'.format(self.dataset_id, url.split('/')[-1])
        sp.run(' '.join([
            'aws', 's3', 'cp',
            url, fn,
            ]),
            shell=True,
            check=True,
            )

    def get_data_url(self, url):
        os.chdir('/data/{:}/bcl'.format(self.dataset_id))
        try:
            sp.run(' '.join([
                'wget', url,
                ]),
                shell=True,
                check=True,
                )
        finally:
            os.chdir('/home/ubuntu')

    def move_output_data_mkfastq(self):
        print('Move mkfastq logs')
        shutil.copytree(
                self.dataset_id,
                '/data/{:}/log/mkfastq/'.format(self.dataset_id),
                )
        print('Remove local mkfastq logs')
        shutil.rmtree(
                self.dataset_id,
                ignore_errors=True)

    def move_output_data_count(self, samplename):
        print('Remove temp files')
        shutil.rmtree(
                self.dataset_id+'/SC_RNA_COUNTER_CS',
                ignore_errors=True)

        print('Move count tables')
        shutil.copytree(
                self.dataset_id+'/outs',
                '/data/{:}/count/{:}'.format(self.dataset_id, samplename),
                )
        shutil.rmtree(
                self.dataset_id+'/outs',
                ignore_errors=True)

        print('Move count logs')
        shutil.copytree(
                self.dataset_id,
                '/data/{:}/log/count/{:}'.format(self.dataset_id, samplename),
                )
        shutil.rmtree(
                self.dataset_id,
                ignore_errors=True)

    def make_count_dataframe(self, samplename):
        import scipy.io
        import pandas as pd

        print('Parse sparse count matrix for sample {:}'.format(samplename))
        fdn_sparse = '/data/{:}/count/{:}/filtered_gene_bc_matrices/mouse_and_mCMV/'.format(
                self.dataset_id,
                samplename,
                )
        fn_sparse = fdn_sparse+'matrix.mtx'
        fn_bc = fdn_sparse+'barcodes.tsv'
        fn_genes = fdn_sparse+'genes.tsv'
        fn_df = fdn_sparse+'dataframe.tsv'

        ind = pd.Index(
                pd.read_csv(fn_genes, sep='\t', header=None).values[:, 0],
                name='Gene')
        col = pd.Index(
                pd.read_csv(fn_bc, sep='\t', squeeze=True, header=None).values,
                name='Cell')

        data = scipy.io.mmread(fn_sparse).todense()

        print('Save dataframe count matrix for sample {:}'.format(samplename))
        df = pd.DataFrame(
                data=data,
                index=ind,
                columns=col,
                )
        df.to_csv(fn_df, sep='\t', index=True)

    def transfer_output_S3(self):
        print('Transfer count logs')
        sp.run(' '.join([
            'aws', 's3', 'cp',
            '--recursive',
            '/data/{:}'.format(self.dataset_id),
            's3://viscrna-seq/data/{:}'.format(self.dataset_id)
            ]),
            shell=True,
            check=True)
        shutil.rmtree(
                '/data/{:}'.format(self.dataset_id),
                ignore_errors=True)

        print('Transfer count table to GDrive')
        print('TODO')

    def __call__(self):
        print('RUN PIPELINE')
        print('Remove old data')
        self.remove_old_data()

        print('Make data folders')
        self.make_data_folders()

        print('Get data')
        self.get_data(self.input_data_path)
        self.input_data_path = '/data/{:}/bcl/{:}'.format(
                self.dataset_id,
                os.path.basename(self.input_data_path))

        print('Unpack data')
        self.unpack_data(self.input_data_path)
        self.input_data_path = '.'.join(self.input_data_path.split('.')[:-2])

        print('Get samplesheet')
        self.get_data(self.input_samplesheet_path)
        self.input_samplesheet_path = '/data/{:}/bcl/{:}'.format(
                self.dataset_id,
                os.path.basename(self.input_samplesheet_path))

        print('Make fastq')
        sp.run(' '.join([
            'cellranger',
            'mkfastq',
            '--id={:}'.format(self.dataset_id),
            '--run={:}'.format(self.input_data_path),
            '--samplesheet={:}'.format(self.input_samplesheet_path),
            '--output-dir=/data/{:}/fastq'.format(self.dataset_id),
            ]),
            shell=True,
            check=True)
        self.move_output_data_mkfastq()

        for samplename in self.samplenames:
            print('Count genes for sample: {:}'.format(samplename))
            sp.run(' '.join([
                'cellranger',
                'count',
                '--id='+self.dataset_id,
                '--transcriptome=/assets/references/mouse_and_mCMV',
                '--fastqs=/data/{:}/fastq'.format(self.dataset_id),
                '--sample={:}'.format(samplename),
                '--expect-cells={:}'.format(self.expect_cells),
                '--chemistry={:}'.format(self.chemistry),
                '--nosecondary',
                ]),
                shell=True,
                check=True)
            # NOTE: output goes in a subfolder of the cwd called like the sample:
            # $HOME/<self.dataset_id>/outs/possorted_genome_bam.bam
            # $HOME/<self.dataset_id>/outs/filtered_gene_bc_matrices_h5.h5
            # see: https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/overview
            self.move_output_data_count(samplename)
            self.make_count_dataframe(samplename)

        print('Transfer data and logs to S3')
        self.transfer_output_S3()
        print('Done')


if __name__ == '__main__':

    pa = argparse.ArgumentParser(description='''viscRNA-Seq pipeline''')
    pa.add_argument(
            '--input-data', required=True,
            help='The location of the raw reads')
    pa.add_argument(
            '--input-samplesheet', required=True,
            help='The location of the samplesheet')
    pa.add_argument(
            '--output', required=True,
            help='The location of the output S3 bucket')
    pa.add_argument(
            '--output-gdrive', required=False, default=None,
            help='The location on GoogleDrive where to store the count table')
    pa.add_argument(
            '--id', required=True,
            help='Sets a unique ID for the dataset, for record keeping')
    pa.add_argument(
            '--samplenames', nargs='+', required=True,
            help='Samplenames in the samplesheet')
    pa.add_argument(
            '--chemistry', required=False, default='threeprime',
            choices=[
                'auto', 'threeprime', 'fiveprime',
                'SC3Pv1', 'SC3Pv2', 'SC5P-PE', 'SC5P-R2',
                ],
            help='Chemistry used in the data, see https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/count')
    pa.add_argument(
            '--expect-cells', required=False, default=3000,
            help='Number of expected cells in the library')

    args = pa.parse_args()

    print('Ready the pipeline')
    pipe = Pipeline(
            input_data_path=args.input_data,
            input_samplesheet_path=args.input_samplesheet,
            output_path=args.output,
            output_gdrive_path=args.output_gdrive,
            dataset_id=args.id,
            samplenames=args.samplenames,
            chemistry=args.chemistry,
            expect_cells=args.expect_cells,
            )

    print('Run the pipeline')
    pipe()
