#!/usr/bin/env python
'''
viscRNA-Seq Pipeline runner.

This script is to be used on user's machines to launch the viscRNA-Seq pipeline
on Amazon EC2 instances in the cloud. Although the script will by default wait
for the pipeline to finish, killing it after the initial launching period has
no consequence on the pipeline itself, which runs asynchronously.

This is an abstract script (!), so it cannot run per se because the pipeline
comes in different flavours, e.g. for different viruses. In order to create a
concrete instance of this script, the AMI_ID, GIT_COMMIT and
GIT_BRANCH global variables must be set to non-empty. The reason I
don't even allow environment variables or the likes is data consistency: only
scripts deployed from CI in a specific branch will and should be usable. No big
data crunching without testing.
'''
import os
import sys
import json
import time
import argparse
import subprocess as sp


AMI_ID = ''
GIT_BRANCH = ''
GIT_COMMIT = ''


class Runner:
    def broadcast_identity(self):
        print('viscRNA-Seq pipeline script: branch {:}, version {:}, AMI {:}'.format(
            GIT_BRANCH,
            GIT_COMMIT,
            AMI_ID,
            ))

    def launch_instance(
            self,
            input_data_path,
            input_samplesheet_path,
            output_path,
            output_gdrive_path,
            dataset_id,
            ):
        tags = ','.join([
            '{Key=Name,Value=viscRNA-Seq-pipeline}',
            '{Key=Project,Value=viscRNA-Seq}',
            '{{Key=datasetId,Value={:}}}'.format(dataset_id),
            ])
        remote_cmd = ' '.join([
            '/usr/local/bin/pipeline',
            '--input-data', input_data_path,
            '--input-samplesheet', input_samplesheet_path,
            '--output', output_path,
            '--output-gdrive', output_gdrive_path,
            '--id', dataset_id,
            ])
        try:
            out = sp.run(' '.join([
                'aws', 'ec2', 'run-instances',
                '--image-id', AMI_ID,
                '--count', '1',
                '--instance-type', 't2.micro',
                '--instance-initiated-shutdown-behavior', 'terminate',
                '--tag-specifications',
                '"ResourceType=instance,Tags=[{:}]"'.format(tags),
                '"ResourceType=volume,Tags=[{:}]"'.format(tags),
                '--user-data', '"'+remote_cmd+'"',
                ]),
                shell=True,
                check=True,
                stdout=sp.PIPE).stdout.decode()
        except sp.CalledProcessError as e:
            print(e.stdout)
            raise

        outd = json.loads(out)
        if ('Instances' not in outd) or (len(outd['Instances']) == 0):
            raise ValueError('No instances launched!')
        instance_id = outd['Instances'][0]['InstanceId']
        instance_state = outd['Instances'][0]['State']['Name']

        self.instance_id = instance_id
        print('EC2 instance id: {:}, {:}'.format(instance_id, instance_state))
        return instance_id

    def monitor_pipeline(self, timeout=40):
        console_output_buffer = []

        t0 = time.time()
        t = time.time()
        while ((t - t0) // 60) < timeout:
            try:
                out = sp.run(' '.join([
                    'aws', 'ec2', 'describe-instances',
                    '--instance-ids', self.instance_id,
                    ]),
                    check=True,
                    shell=True).stdout
            except sp.CalledProcessError as e:
                if e.stdout is not None:
                    print(e.stdout.decode())
                self.terminate_instance(self.instance_id)
                raise

            outd = json.loads(out.decode())
            if ('Instances' not in outd) or (len(outd['Instances']) == 0):
                self.terminate_instance(self.instance_id)
                raise ValueError('No instances launched!')

            instance_state = outd['Instances'][0]['State']['Name']
            print('Job state: {:}'.format(instance_state))

            if instance_state != 'running':
                time.sleep(30)
                t = time.time()
                continue

            # Print console output
            try:
                out = sp.run(' '.join([
                    'aws', 'ec2', 'get-console-output',
                    '--instance-id', self.instance_id,
                    ]),
                    check=True,
                    shell=True).stdout
            except sp.CalledProcessError as e:
                if e.stdout is not None:
                    print(e.stdout.decode())
                self.terminate_instance(self.instance_id)
                raise

            lines = json.loads(out.decode())['Output'].split('\n')
            if len(lines) > 0:
                # Check in buffer
                for il, line in enumerate(console_output_buffer):
                    if line == lines[0]:
                        n_overlap = len(console_output_buffer) - il
                        if len(lines) <= n_overlap:
                            lines = []
                        else:
                            lines = lines[n_overlap:]
                        break

                for line in lines:
                    print(line)

                console_output_buffer.extend(lines)

                if len(console_output_buffer) > 50_000:
                    console_output_buffer = console_output_buffer[-50_000:]

            time.sleep(30)
            t = time.time()

    def terminate_instance(self, instance_id):
        sp.run(' '.join([
            'aws', 'ec2',
            'terminate-instances',
            '--instance-ids', instance_id,
            ]),
            shell=True,
            check=True,
        )


if __name__ == '__main__':

    # FIXME: more specific help for the locations
    pa = argparse.ArgumentParser(description='''viscRNA-Seq pipeline''')
    spas = pa.add_subparsers(dest='subparser_name')

    par = spas.add_parser('run')
    par.add_argument(
            '--input-data', required=True,
            help='The location of the raw reads')
    par.add_argument(
            '--input-samplesheet', required=True,
            help='The location of the samplesheet')
    par.add_argument(
            '--output', required=True,
            help='The location of the output S3 bucket')
    par.add_argument(
            '--output-gdrive', required=False, default=None,
            help='The location on GoogleDrive where to store the count table')
    par.add_argument(
            '--id', required=True,
            help='Sets a unique ID for the dataset, for record keeping')
    par.add_argument(
            '--monitor', type=int, default=None,
            help='Monitoring the pipeline after launch for X minutes (0 means forever)')

    pam = spas.add_parser('monitor')
    pam.add_argument(
            '--instance-id', required=True,
            help='The id of the EC2 instance')
    pam.add_argument(
            '--timeout', type=int, default=0,
            help='Monitoring the pipeline after launch for X minutes (0 means forever)')

    pas = spas.add_parser('terminate')
    pas.add_argument(
            '--instance-id', required=True,
            help='The id of the EC2 instance')

    args = pa.parse_args()

    run = Runner()
    run.broadcast_identity()

    if args.subparser_name == 'run':
        instance_id = run.launch_instance(
                input_data_path=args.input_data,
                input_samplesheet_path=args.input_samplesheet,
                output_path=args.output,
                output_gdrive_path=args.output_gdrive,
                dataset_id=args.id,
                )

        if args.monitor is not None:
            run.monitor_pipeline(timeout=args.monitor)

    elif args.subparser_name == 'monitor':
        run.instance_id = args.instance_id
        run.monitor_pipeline(timeout=args.timeout)

    elif args.subparser_name == 'terminate':
        run.terminate_instance(args.instance_id)

    else:
        raise ValueError('Subcommand not found: {:}'.format(args.subparser_name))
